{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6848522f-31a6-4c89-8382-51f2e29b215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import base64\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import threading\n",
    "import argparse\n",
    "import re\n",
    "import fcntl\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf6e23-5377-4932-88c6-e0f41cc0a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e08b22-67d4-4851-9798-3bafbc2af61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configurations\n",
    "MODEL_CONFIGS = [\n",
    "    # 高并发\n",
    "    {\"name\": \"qwen-vl-plus-latest\", \"token_limit\": 20000000, \"max_workers\": 13, \"request_interval\": 0.09, \"input_rate_per_k\": 0.0015, \"output_rate_per_k\": 0.0045},\n",
    "    # Add more models as needed, including request_interval, input_rate_per_k, output_rate_per_k\n",
    "\n",
    "    # 低并发\n",
    "    # Add more models as needed, including request_interval, input_rate_per_k, output_rate_per_k\n",
    "\n",
    "    # 已用尽\n",
    "    # {'name': \"qwen2.5-vl-3b-instruct\", \"token_limit\": 900000, \"max_workers\": 10, \"request_interval\": 0.2, \"input_rate_per_k\": 0.0012, \"output_rate_per_k\": 0.0036},\n",
    "    # {'name': \"qwen2.5-vl-7b-instruct\", \"token_limit\": 270000, \"max_workers\": 10, \"request_interval\": 0.2, \"input_rate_per_k\": 0.002, \"output_rate_per_k\": 0.005},\n",
    "    # {\"name\": \"qwen-vl-max\", \"token_limit\": 600000, \"max_workers\": 20, \"request_interval\": 0.2, \"input_rate_per_k\": 0.003, \"output_rate_per_k\": 0.009},\n",
    "    # {'name': \"qwen2.5-vl-32b-instruct\", \"token_limit\": 110000, \"max_workers\": 4, \"request_interval\": 0.8, \"input_rate_per_k\": 0.008, \"output_rate_per_k\": 0.024},\n",
    "    # {'name': \"qwen2.5-vl-72b-instruct\", \"token_limit\": 650000, \"max_workers\": 4, \"request_interval\": 0.8, \"input_rate_per_k\": 0.016, \"output_rate_per_k\": 0.048},\n",
    "    # {'name': \"qwen-vl-plus-latest\", \"token_limit\": 800000, \"max_workers\": 20, \"request_interval\": 0.2, \"input_rate_per_k\": 0.0015, \"output_rate_per_k\": 0.0045},\n",
    "    # {'name': \"qwen-vl-max-latest\", \"token_limit\": 450000, \"max_workers\": 20, \"request_interval\": 0.2, \"input_rate_per_k\": 0.003, \"output_rate_per_k\": 0.009},\n",
    "    # {\"name\": \"qwen-vl-plus-2025-01-25\", \"token_limit\": 70000, \"max_workers\": 3, \"request_interval\": 0.8, \"input_rate_per_k\": 0.0015, \"output_rate_per_k\": 0.0045},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41086d46-ecb2-4fca-a706-d55723f1421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并发控制设置 (These are now default/fallback, primary control is per-model)\n",
    "# MAX_WORKERS = 10 # Dynamically set\n",
    "MAX_RETRIES = 3  # 最大重试次数\n",
    "RETRY_DELAY = 2  # 重试延迟（秒）\n",
    "# REQUEST_INTERVAL = 0.2  # Now defined per model\n",
    "MIN_IMAGES_REQUIRED = 4  # 子文件夹中需要的最少图像数量\n",
    "\n",
    "# Token/Model State Management\n",
    "current_model_index = 0\n",
    "model_token_usage = {config[\"name\"]: 0 for config in MODEL_CONFIGS}\n",
    "all_models_exhausted = False\n",
    "current_max_workers = MODEL_CONFIGS[current_model_index][\"max_workers\"] if MODEL_CONFIGS else 10 # Default if no configs\n",
    "\n",
    "# 全局请求控制\n",
    "request_lock = threading.Lock()\n",
    "last_request_time = time.time()\n",
    "error_count = 0\n",
    "consecutive_errors_threshold = 5\n",
    "\n",
    "# Token统计\n",
    "total_tokens_used = 0\n",
    "total_requests = 0\n",
    "token_lock = threading.Lock()\n",
    "\n",
    "# 费用统计（按照提供的费率） - Now defined per model\n",
    "# INPUT_RATE_PER_K = 0.0015\n",
    "# OUTPUT_RATE_PER_K = 0.0045\n",
    "\n",
    "# JSON文件锁\n",
    "json_file_lock = threading.Lock()\n",
    "\n",
    "# 数据集收集 - 用于内存中缓存\n",
    "dataset = []\n",
    "processed_subfolders = set()  # 用于跟踪已处理的子文件夹\n",
    "dataset_lock = threading.Lock()\n",
    "\n",
    "# 设置日志\n",
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "log_file = f\"./logs/scene_change_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# 添加控制台日志处理器，但限制输出\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.WARNING)  # 只显示警告和错误\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "# 预设类别列表\n",
    "# CATEGORIES = [\"lake_or_pond\", \"waste_disposal\", \"lighthouse\", \"road_bridge\", \"tower\", \"swimming_pool\", \"smokestack\", \"educational_institution\", \"amusement_park\", \"park\", \"stadium\", \"construction_site\", \"tunnel_opening\", \"space_facility\", \"railway_bridge\"]\n",
    "\n",
    "# CATEGORIES = [\"barn\", \"race_track\", \"nuclear_powerplant\", \"place_of_worship\", \"shopping_mall\", \"runway\", \"ground_transportation_station\", \"fire_station\", \"solar_farm\", \"oil_or_gas_facility\", \"police_station\", \"surface_mine\", \"archaeological_site\", \"office_building\", \"recreational_facility\", \"border_checkpoint\", \"interchange\", \"hospital\", \"multi-unit_residential\", \"debris_or_rubble\", \"factory_or_powerplant\", \"parking_lot_or_garage\"]\n",
    "\n",
    "CATEGORIES = [\"fountain\", \"electric_substation\", \"water_treatment_facility\", \"car_dealership\", \"crop_field\", \"wind_farm\", \"helipad\", \"gas_station\", \"impoverished_settlement\", \"airport_terminal\", \"golf_course\", \"single-unit_residential\", \"dam\", \"burial_site\", \"airport\", \"flooded_road\", \"shipyard\", \"zoo\", \"airport_hangar\", \"military_facility\", \"toll_booth\", \"aquaculture\", \"port\", \"prison\", \"storage_tank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25726426-c680-476c-b4df-42787c88c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_model_config():\n",
    "    \"\"\"Gets the configuration for the currently active model.\"\"\"\n",
    "    if all_models_exhausted or current_model_index >= len(MODEL_CONFIGS):\n",
    "        return None\n",
    "    return MODEL_CONFIGS[current_model_index]\n",
    "\n",
    "def switch_to_next_model():\n",
    "    \"\"\"Switches to the next available model in the list.\"\"\"\n",
    "    global current_model_index, all_models_exhausted, current_max_workers\n",
    "    with token_lock: # Use token_lock for thread safety when modifying shared state\n",
    "        current_model_index += 1\n",
    "        if current_model_index >= len(MODEL_CONFIGS):\n",
    "            all_models_exhausted = True\n",
    "            logging.warning(\"All models have reached their token limits. Stopping processing.\")\n",
    "            print(\"All models have reached their token limits. Stopping processing.\")\n",
    "            current_max_workers = 1 # Reduce workers if all exhausted\n",
    "        else:\n",
    "            new_model_config = MODEL_CONFIGS[current_model_index]\n",
    "            current_max_workers = new_model_config[\"max_workers\"]\n",
    "            logging.info(f\"Switching to model: {new_model_config['name']} (Limit: {new_model_config['token_limit']}, Workers: {current_max_workers})\")\n",
    "            print(f\"Switching to model: {new_model_config['name']} (Limit: {new_model_config['token_limit']}, Workers: {current_max_workers})\")\n",
    "    return not all_models_exhausted\n",
    "\n",
    "def calculate_image_tokens(image_path):\n",
    "    \"\"\"计算图像的token数量\"\"\"\n",
    "    try:\n",
    "        # 打开图片文件\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # 获取图片的原始尺寸\n",
    "        height = image.height\n",
    "        width = image.width\n",
    "        \n",
    "        # 将高度调整为28的整数倍\n",
    "        h_bar = round(height / 28) * 28\n",
    "        # 将宽度调整为28的整数倍\n",
    "        w_bar = round(width / 28) * 28\n",
    "        \n",
    "        # 图像的Token下限：4个Token\n",
    "        min_pixels = 28 * 28 * 4\n",
    "        # 图像的Token上限：1280个Token\n",
    "        max_pixels = 1280 * 28 * 28\n",
    "            \n",
    "        # 对图像进行缩放处理，调整像素的总数在范围[min_pixels,max_pixels]内\n",
    "        if h_bar * w_bar > max_pixels:\n",
    "            # 计算缩放因子beta，使得缩放后的图像总像素数不超过max_pixels\n",
    "            beta = math.sqrt((height * width) / max_pixels)\n",
    "            # 重新计算调整后的高度，确保为28的整数倍\n",
    "            h_bar = math.floor(height / beta / 28) * 28\n",
    "            # 重新计算调整后的宽度，确保为28的整数倍\n",
    "            w_bar = math.floor(width / beta / 28) * 28\n",
    "        elif h_bar * w_bar < min_pixels:\n",
    "            # 计算缩放因子beta，使得缩放后的图像总像素数不低于min_pixels\n",
    "            beta = math.sqrt(min_pixels / (height * width))\n",
    "            # 重新计算调整后的高度，确保为28的整数倍\n",
    "            h_bar = math.ceil(height * beta / 28) * 28\n",
    "            # 重新计算调整后的宽度，确保为28的整数倍\n",
    "            w_bar = math.ceil(width * beta / 28) * 28\n",
    "        \n",
    "        # 计算图像的Token数：总像素除以28 * 28，加上标记的2个Token\n",
    "        token_count = int((h_bar * w_bar) / (28 * 28)) + 2\n",
    "        return token_count\n",
    "    except Exception as e:\n",
    "        logging.error(f\"计算图片token失败: {str(e)}\")\n",
    "        return 1282  # 返回可能的最大值作为保守估计\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"将图片编码为base64字符串，用于API请求\"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"编码图片 {image_path} 失败: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def wait_for_request_interval():\n",
    "    \"\"\"确保请求之间的最小间隔 (uses current model's interval)\"\"\"\n",
    "    global last_request_time\n",
    "    model_config = get_current_model_config()\n",
    "    # Use a default interval if somehow no model is configured (should not happen)\n",
    "    current_interval = model_config[\"request_interval\"] if model_config else 0.2\n",
    "\n",
    "    with request_lock:\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_request_time\n",
    "        if elapsed < current_interval:\n",
    "            sleep_time = current_interval - elapsed\n",
    "            time.sleep(sleep_time)\n",
    "        last_request_time = time.time()\n",
    "\n",
    "def update_token_stats(image_tokens, completion_tokens):\n",
    "    \"\"\"更新token统计信息，并检查当前模型是否超过限制 (uses current model's rates)\"\"\"\n",
    "    global total_tokens_used, total_requests, model_token_usage\n",
    "\n",
    "    model_config = get_current_model_config()\n",
    "    if not model_config:\n",
    "        return True # Stop if no model is available\n",
    "\n",
    "    current_model_name = model_config[\"name\"]\n",
    "    current_model_limit = model_config[\"token_limit\"]\n",
    "    # Get rates from the current model config, provide defaults if missing\n",
    "    input_rate = model_config.get(\"input_rate_per_k\", 0.0015)\n",
    "    output_rate = model_config.get(\"output_rate_per_k\", 0.0045)\n",
    "\n",
    "    request_tokens = image_tokens + completion_tokens\n",
    "\n",
    "    with token_lock:\n",
    "        # Update overall stats\n",
    "        total_tokens_used += request_tokens\n",
    "        total_requests += 1\n",
    "        avg_tokens = total_tokens_used / total_requests if total_requests > 0 else 0\n",
    "\n",
    "        # Update current model's usage\n",
    "        previous_model_usage = model_token_usage[current_model_name]\n",
    "        model_token_usage[current_model_name] += request_tokens\n",
    "\n",
    "        # Calculate costs using model-specific rates\n",
    "        input_cost = (image_tokens / 1000) * input_rate\n",
    "        output_cost = (completion_tokens / 1000) * output_rate\n",
    "        request_cost = input_cost + output_cost\n",
    "        # Total cost estimate is tricky without knowing token split per model, use average of first model?\n",
    "        # Or maybe log per-model cost instead? Let's just log the request cost for now.\n",
    "        # TODO: Improve total cost estimation if needed.\n",
    "        # total_cost_estimate = (total_tokens_used / 1000) * (INPUT_RATE_PER_K + OUTPUT_RATE_PER_K) / 2\n",
    "\n",
    "        logging.info(f\"Model: {current_model_name} - Token统计: Img {image_tokens}, Comp {completion_tokens}, \"\n",
    "                     f\"Model Total {model_token_usage[current_model_name]}/{current_model_limit}, \"\n",
    "                     f\"Overall Total {total_tokens_used}, Avg Req {avg_tokens:.1f}, \"\n",
    "                     f\"Cost: Req ¥{request_cost:.4f} (Rates: In {input_rate}, Out {output_rate})\")\n",
    "\n",
    "        # Check if the current model's limit is exceeded\n",
    "        if model_token_usage[current_model_name] >= current_model_limit and previous_model_usage < current_model_limit:\n",
    "            logging.warning(f\"Model {current_model_name} reached token limit ({current_model_limit}). Usage: {model_token_usage[current_model_name]}.\")\n",
    "            print(f\"Model {current_model_name} reached token limit ({current_model_limit}).\")\n",
    "            if not switch_to_next_model():\n",
    "                return True # Signal that all models are exhausted\n",
    "\n",
    "        return all_models_exhausted # Return whether processing should stop overall\n",
    "\n",
    "def should_stop_processing():\n",
    "    \"\"\"Checks if all models have reached their token limits.\"\"\"\n",
    "    return all_models_exhausted\n",
    "\n",
    "def get_date_from_filename(filename):\n",
    "    \"\"\"从文件名中提取日期\"\"\"\n",
    "    match = re.search(r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', filename)\n",
    "    if match:\n",
    "        year, month, day = map(int, match.groups())\n",
    "        return datetime(year, month, day)\n",
    "    return None\n",
    "\n",
    "def sort_images_by_date(image_paths):\n",
    "    \"\"\"按照文件名中的日期对图像进行排序\"\"\"\n",
    "    return sorted(image_paths, key=lambda x: get_date_from_filename(os.path.basename(x)) or datetime.min)\n",
    "\n",
    "def read_scene_description(subfolder):\n",
    "    \"\"\"读取场景描述文件的内容\"\"\"\n",
    "    desc_path = os.path.join(subfolder, \"scene_description.txt\")\n",
    "    if os.path.exists(desc_path):\n",
    "        try:\n",
    "            with open(desc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read().strip()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"无法读取场景描述文件 {desc_path}: {str(e)}\")\n",
    "    return \"\"\n",
    "\n",
    "def load_existing_dataset(json_file_path):\n",
    "    \"\"\"加载现有的数据集文件，用于续传\"\"\"\n",
    "    global dataset, processed_subfolders\n",
    "    \n",
    "    if os.path.exists(json_file_path) and os.path.getsize(json_file_path) > 0:\n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                loaded_data = json.load(f)\n",
    "                if isinstance(loaded_data, list):\n",
    "                    with dataset_lock:\n",
    "                        dataset = loaded_data\n",
    "                        # 从现有数据集构建已处理子文件夹集合\n",
    "                        for item in dataset:\n",
    "                            if 'output_image' in item:\n",
    "                                subfolder = os.path.dirname(item['output_image'])\n",
    "                                processed_subfolders.add(subfolder)\n",
    "                    \n",
    "                    print(f\"已加载现有数据集，包含 {len(dataset)} 条记录\")\n",
    "                    logging.info(f\"已加载现有数据集，包含 {len(dataset)} 条记录\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"警告: 数据集文件 {json_file_path} 格式不正确\")\n",
    "                    logging.warning(f\"数据集文件 {json_file_path} 格式不正确\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"警告: 无法解析数据集文件 {json_file_path}: {str(e)}\")\n",
    "            logging.warning(f\"无法解析数据集文件 {json_file_path}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"警告: 读取数据集文件 {json_file_path} 时出错: {str(e)}\")\n",
    "            logging.warning(f\"读取数据集文件 {json_file_path} 时出错: {str(e)}\")\n",
    "    \n",
    "    # 如果文件不存在或读取失败，初始化为空列表\n",
    "    with dataset_lock:\n",
    "        dataset = []\n",
    "        processed_subfolders = set()\n",
    "    return False\n",
    "\n",
    "def is_subfolder_processed(subfolder):\n",
    "    \"\"\"检查子文件夹是否已经处理过\"\"\"\n",
    "    with dataset_lock:\n",
    "        return subfolder in processed_subfolders\n",
    "\n",
    "def add_to_dataset(input_images, output_image, input_prompt, output_description, json_file_path):\n",
    "    \"\"\"添加条目到数据集并立即写入JSON文件\"\"\"\n",
    "    global dataset, processed_subfolders\n",
    "    \n",
    "    # 创建新条目\n",
    "    new_entry = {\n",
    "        \"input_images\": input_images,\n",
    "        \"output_image\": output_image,\n",
    "        \"input_prompt\": input_prompt,\n",
    "        \"output_description\": output_description\n",
    "    }\n",
    "    \n",
    "    # 添加到内存中的数据集\n",
    "    with dataset_lock:\n",
    "        dataset.append(new_entry)\n",
    "        processed_subfolders.add(os.path.dirname(output_image))\n",
    "    \n",
    "    # 立即写入JSON文件\n",
    "    with json_file_lock:\n",
    "        try:\n",
    "            temp_file = f\"{json_file_path}.temp\"\n",
    "            with open(temp_file, 'w', encoding='utf-8') as f:\n",
    "                # 获取文件锁，确保写入操作的原子性\n",
    "                fcntl.flock(f, fcntl.LOCK_EX)\n",
    "                try:\n",
    "                    # 使用内存中的完整数据集写入临时文件\n",
    "                    with dataset_lock:\n",
    "                        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "                    # 确保数据写入磁盘\n",
    "                    f.flush()\n",
    "                    os.fsync(f.fileno())\n",
    "                finally:\n",
    "                    # 释放文件锁\n",
    "                    fcntl.flock(f, fcntl.LOCK_UN)\n",
    "            \n",
    "            # 重命名临时文件，替换原文件\n",
    "            os.replace(temp_file, json_file_path)\n",
    "            logging.info(f\"已将新条目保存到数据集文件，当前共 {len(dataset)} 条记录\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"保存数据集时发生错误: {str(e)}\")\n",
    "            if os.path.exists(temp_file):\n",
    "                try:\n",
    "                    os.remove(temp_file)\n",
    "                except:\n",
    "                    pass\n",
    "            return False\n",
    "\n",
    "def get_change_description(images, prompt, subfolder_path, folder_name):\n",
    "    \"\"\"从模型获取图像变化描述\"\"\"\n",
    "    global error_count\n",
    "\n",
    "    # Check if processing should stop overall\n",
    "    if should_stop_processing():\n",
    "        logging.info(f\"Skipping request for {subfolder_path} (All models exhausted)\")\n",
    "        return None\n",
    "\n",
    "    model_config = get_current_model_config()\n",
    "    if not model_config: # Should not happen if should_stop_processing is checked first, but good practice\n",
    "        logging.error(f\"No available model for request {subfolder_path}\")\n",
    "        return None\n",
    "\n",
    "    current_model_name = model_config[\"name\"]\n",
    "    current_model_limit = model_config[\"token_limit\"]\n",
    "    current_model_tokens_used = model_token_usage[current_model_name]\n",
    "\n",
    "    # 验证所有图片\n",
    "    valid_images = []\n",
    "    for img_path in images:\n",
    "        if os.path.exists(img_path) and os.path.getsize(img_path) > 0:\n",
    "            base64_image = encode_image_to_base64(img_path)\n",
    "            if base64_image:\n",
    "                valid_images.append((img_path, base64_image))\n",
    "        else:\n",
    "            logging.warning(f\"图片不存在或大小为0: {img_path}\")\n",
    "    \n",
    "    if len(valid_images) < len(images):\n",
    "        logging.error(f\"子文件夹 {subfolder_path} 中有无效图片，跳过处理\")\n",
    "        return None\n",
    "    \n",
    "    # 计算图像token\n",
    "    total_image_tokens = sum([calculate_image_tokens(img_path) for img_path, _ in valid_images])\n",
    "\n",
    "    # 检查当前模型单个请求是否可能超过限制\n",
    "    with token_lock:\n",
    "        # Estimate completion tokens conservatively (e.g., 100)\n",
    "        estimated_completion_tokens = 100\n",
    "        expected_tokens_for_request = total_image_tokens + estimated_completion_tokens\n",
    "        if current_model_tokens_used + expected_tokens_for_request > current_model_limit:\n",
    "            logging.warning(f\"Skipping request {subfolder_path} for model {current_model_name} (Estimated request would exceed token limit)\")\n",
    "            # Attempt to switch model immediately if this one is likely full\n",
    "            if not switch_to_next_model():\n",
    "                # If switching fails (all models exhausted), return None\n",
    "                 return None\n",
    "            else:\n",
    "                 # If switching succeeds, let the next iteration/task try with the new model\n",
    "                 # Returning None here prevents processing this subfolder *now*\n",
    "                 # but allows the overall process to continue if a new model is available.\n",
    "                 # Alternatively, could retry the request immediately with the new model,\n",
    "                 # but that adds complexity. Let's stick to skipping for now.\n",
    "                 return None\n",
    "\n",
    "    # 准备消息内容\n",
    "    content = []\n",
    "    \n",
    "    # 添加所有图片\n",
    "    for img_path, base64_image in valid_images:\n",
    "        content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # 添加文本提示\n",
    "    change_prompt = f\"Scene: {folder_name}. {prompt} In under 77 words, describe specific changes between these images (earliest to latest) in English. Focus on precise location details (e.g., 'new building in bottom-left', 'runway extended northward'). Identify concrete changes in structures, landscape, or development. Be specific and concise. Must be within 50 words. Attention!! Focus on changes not description.\"\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": change_prompt\n",
    "    })\n",
    "    \n",
    "    # 记录请求信息\n",
    "    logging.info(f\"Preparing change description request for {subfolder_path} using model {current_model_name} ({len(valid_images)} images)\")\n",
    "\n",
    "    # 添加重试机制\n",
    "    for retry in range(MAX_RETRIES):\n",
    "        # Check again before each attempt\n",
    "        if should_stop_processing():\n",
    "            logging.info(f\"Canceling request for {subfolder_path} (All models exhausted)\")\n",
    "            return None\n",
    "\n",
    "        # Check if the *specific model* changed or became exhausted during wait/retry\n",
    "        model_config_before_wait = get_current_model_config()\n",
    "        if not model_config_before_wait or model_config_before_wait[\"name\"] != current_model_name:\n",
    "             logging.info(f\"Model changed during retry/wait for {subfolder_path}. Aborting current attempt.\")\n",
    "             # Let the caller handle retrying with the potentially new model if needed.\n",
    "             return None # Indicate failure for this attempt with the old model\n",
    "\n",
    "        try:\n",
    "            # 等待请求间隔\n",
    "            wait_for_request_interval()\n",
    "            \n",
    "            # 创建OpenAI客户端\n",
    "            client = OpenAI(\n",
    "                base_url=BASE_URL,\n",
    "                api_key=OPENROUTER_API_KEY,\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Sending change description request: {subfolder_path} with model {current_model_name} (Try {retry+1}/{MAX_RETRIES})\")\n",
    "            \n",
    "            # 发送请求\n",
    "            response = client.chat.completions.create(\n",
    "                model=current_model_name, # Use the current model name\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=256,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            # 提取描述内容\n",
    "            description = None\n",
    "            completion_tokens = 0\n",
    "            \n",
    "            # 从响应中提取数据\n",
    "            if hasattr(response, 'model_dump_json'):\n",
    "                response_json = json.loads(response.model_dump_json())\n",
    "                if 'choices' in response_json and response_json['choices'] and 'message' in response_json['choices'][0]:\n",
    "                    message = response_json['choices'][0]['message']\n",
    "                    if 'content' in message and message['content']:\n",
    "                        description = message['content']\n",
    "                if 'usage' in response_json and 'completion_tokens' in response_json['usage']:\n",
    "                    completion_tokens = response_json['usage']['completion_tokens']\n",
    "            \n",
    "            # 备用方案：直接访问对象属性\n",
    "            if description is None and hasattr(response, 'choices') and response.choices and len(response.choices) > 0:\n",
    "                choice = response.choices[0]\n",
    "                if hasattr(choice, 'message') and hasattr(choice.message, 'content'):\n",
    "                    description = choice.message.content\n",
    "            \n",
    "            # 如果获取到描述\n",
    "            if description:\n",
    "                # 更新token统计，检查是否超过限制\n",
    "                token_exceeded = update_token_stats(total_image_tokens, completion_tokens)\n",
    "                \n",
    "                # 重置错误计数\n",
    "                with request_lock:\n",
    "                    error_count = 0\n",
    "                \n",
    "                return description\n",
    "            \n",
    "            # 如果所有方法都无法提取描述\n",
    "            logging.error(f\"无法从响应中提取变化描述内容\")\n",
    "            \n",
    "            # 增加错误计数\n",
    "            with request_lock:\n",
    "                error_count += 1\n",
    "                current_errors = error_count\n",
    "                \n",
    "            # 检查是否达到连续错误阈值\n",
    "            if current_errors >= consecutive_errors_threshold:\n",
    "                logging.warning(f\"检测到连续 {current_errors} 次错误，暂停5秒后继续...\")\n",
    "                time.sleep(5)\n",
    "                with request_lock:\n",
    "                    error_count = 0\n",
    "            \n",
    "            raise ValueError(\"无法从API响应中提取描述内容\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Model {current_model_name} request failed for {subfolder_path} (Try {retry+1}/{MAX_RETRIES}): {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "\n",
    "            if retry < MAX_RETRIES - 1:\n",
    "                # 添加随机延迟再重试\n",
    "                delay = RETRY_DELAY + random.uniform(0, 2)\n",
    "                logging.info(f\"等待 {delay:.2f} 秒后重试...\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    logging.error(f\"Failed to get change description for {subfolder_path} with model {current_model_name} after {MAX_RETRIES} tries.\")\n",
    "    return None\n",
    "\n",
    "def process_subfolder(subfolder, category, prompt, output_json):\n",
    "    \"\"\"处理单个子文件夹，生成变化描述并添加到数据集\"\"\"\n",
    "    # Check if processing should stop overall\n",
    "    if should_stop_processing():\n",
    "        return \"TOKEN_EXCEEDED\" # Use existing status for simplicity\n",
    "\n",
    "    try:\n",
    "        # 检查内存中是否标记为已处理\n",
    "        if is_subfolder_processed(subfolder):\n",
    "            logging.info(f\"跳过已处于数据集中的子文件夹: {subfolder}\")\n",
    "            return \"ALREADY_PROCESSED\"\n",
    "            \n",
    "        # 提取文件夹名称\n",
    "        folder_name = os.path.basename(subfolder)\n",
    "        \n",
    "        # 检查是否已经处理过（断点续传）\n",
    "        output_file = os.path.join(subfolder, \"scene_change.txt\")\n",
    "        if os.path.exists(output_file):\n",
    "            logging.info(f\"跳过已处理的子文件夹: {subfolder}\")\n",
    "            return \"ALREADY_PROCESSED\"  # 返回特殊状态表示已处理\n",
    "        \n",
    "        # 获取该子文件夹下的所有jpg图片\n",
    "        image_paths = glob.glob(os.path.join(subfolder, \"*.jpg\"))\n",
    "        \n",
    "        # 检查是否有足够的图像\n",
    "        if len(image_paths) < MIN_IMAGES_REQUIRED:\n",
    "            logging.warning(f\"子文件夹 {subfolder} 图像数量不足 ({len(image_paths)}/{MIN_IMAGES_REQUIRED})，跳过处理\")\n",
    "            return \"INSUFFICIENT_IMAGES\"\n",
    "        \n",
    "        # 按日期排序图像\n",
    "        sorted_images = sort_images_by_date(image_paths)\n",
    "        \n",
    "        # 选择最晚的4张图像\n",
    "        selected_images = sorted_images[-MIN_IMAGES_REQUIRED:]\n",
    "        \n",
    "        # 分割成输入图像和输出图像\n",
    "        input_images = selected_images[:3]\n",
    "        output_image = selected_images[3]\n",
    "        \n",
    "        logging.info(f\"处理子文件夹: {subfolder}, 选择最晚的 {MIN_IMAGES_REQUIRED} 张图片进行变化描述\")\n",
    "        \n",
    "        # 读取场景描述作为输入提示\n",
    "        input_prompt = read_scene_description(subfolder)\n",
    "        \n",
    "        # 调用模型获取变化描述\n",
    "        change_description = get_change_description(selected_images, prompt, subfolder, folder_name)\n",
    "\n",
    "        # Check again after the potentially long API call\n",
    "        if should_stop_processing():\n",
    "             # If limits were hit *during* the API call, the result might be valid,\n",
    "             # but we shouldn't continue submitting new tasks.\n",
    "             # We still process the *result* of this completed task if successful.\n",
    "             pass # Let the logic below handle the result, but the outer loop will stop\n",
    "\n",
    "        if change_description:\n",
    "            # 保存描述到txt文件\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(change_description)\n",
    "            logging.info(f\"变化描述已保存到: {output_file}\")\n",
    "            \n",
    "            # 添加到数据集并立即保存\n",
    "            add_to_dataset(input_images, output_image, input_prompt, change_description, output_json)\n",
    "            \n",
    "            return subfolder\n",
    "        else:\n",
    "            # Log failure, but check if it was due to token limits being hit *globally*\n",
    "            if should_stop_processing():\n",
    "                 logging.warning(f\"Failed to get description for {subfolder}, likely due to hitting token limits.\")\n",
    "                 return \"TOKEN_EXCEEDED\" # Signal to stop\n",
    "            else:\n",
    "                 # Failure occurred within get_change_description.\n",
    "                 # It could be due to hitting token limits (which sets all_models_exhausted)\n",
    "                 # or other API errors after retries.\n",
    "                 # Log the failure for this specific subfolder.\n",
    "                 logging.error(f\"无法为 {subfolder} 获取变化描述 (model might be exhausted or other API error occurred)\")\n",
    "                 # Check if the global stop flag is set *now*. If so, signal to stop processing further tasks.\n",
    "                 if should_stop_processing():\n",
    "                     return \"TOKEN_EXCEEDED\"\n",
    "                 else:\n",
    "                     # Otherwise, it was a non-limit-related failure for this subfolder, return None to indicate failure for this item.\n",
    "                     return None # Signal failure for this specific subfolder\n",
    "    except Exception as e:\n",
    "        logging.error(f\"处理子文件夹 {subfolder} 时发生异常: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_dataset(dataset_path, categories=None, prompt=\"\", output_json=\"dataset.json\", use_parallel=True):\n",
    "    \"\"\"处理指定类别的数据集，生成变化描述和构建数据集文件\"\"\"\n",
    "    global current_max_workers # Allow modification if model switches\n",
    "\n",
    "    # 加载现有数据集（续传功能）\n",
    "    load_existing_dataset(output_json)\n",
    "    \n",
    "    if not MODEL_CONFIGS:\n",
    "        print(\"错误: MODEL_CONFIGS is empty. Please define models to use.\")\n",
    "        logging.error(\"MODEL_CONFIGS is empty.\")\n",
    "        return\n",
    "\n",
    "    initial_model_config = get_current_model_config()\n",
    "    print(f\"Starting with model: {initial_model_config['name']} (Limit: {initial_model_config['token_limit']}, Workers: {initial_model_config['max_workers']})\")\n",
    "    logging.info(f\"Starting processing with model: {initial_model_config['name']}\")\n",
    "\n",
    "    # 初始化计数器\n",
    "    total_processed = 0\n",
    "    total_success = 0\n",
    "    total_skipped = 0\n",
    "    total_insufficient = 0\n",
    "    \n",
    "    # 获取所有类别文件夹或使用指定的类别\n",
    "    all_categories = os.listdir(dataset_path) if not categories else categories\n",
    "    \n",
    "    # 过滤出有效的类别文件夹\n",
    "    valid_categories = []\n",
    "    for category in all_categories:\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            valid_categories.append(category)\n",
    "    \n",
    "    if not valid_categories:\n",
    "        print(f\"错误: 在 {dataset_path} 中未找到有效的类别文件夹\")\n",
    "        return\n",
    "    \n",
    "    print(f\"将处理以下类别: {', '.join(valid_categories)}\")\n",
    "    logging.info(f\"将处理以下类别: {', '.join(valid_categories)}\")\n",
    "    \n",
    "    for category in valid_categories:\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        \n",
    "        print(f\"处理类别: {category}\")\n",
    "        logging.info(f\"处理类别: {category}\")\n",
    "        \n",
    "        # 获取该类别下的所有子文件夹\n",
    "        subfolders = [os.path.join(category_path, d) for d in os.listdir(category_path) \n",
    "                      if os.path.isdir(os.path.join(category_path, d))]\n",
    "        \n",
    "        if not subfolders:\n",
    "            continue\n",
    "            \n",
    "        # 创建进度条\n",
    "        pbar = tqdm(total=len(subfolders), desc=f\"Processing {category} (Model: {get_current_model_config()['name'] if get_current_model_config() else 'N/A'})\")\n",
    "\n",
    "        # Check if processing should stop before starting category\n",
    "        if should_stop_processing():\n",
    "            print(f\"All models exhausted. Stopping before processing category {category}.\")\n",
    "            break\n",
    "\n",
    "        if use_parallel:\n",
    "            # 并行处理\n",
    "            # Use current_max_workers which might change if models switch\n",
    "            with ThreadPoolExecutor(max_workers=current_max_workers) as executor:\n",
    "                futures = []\n",
    "                future_to_subfolder = {}\n",
    "\n",
    "                for subfolder in subfolders:\n",
    "                    # Check before submitting each task\n",
    "                    if should_stop_processing():\n",
    "                        logging.warning(\"Stopping task submission, models exhausted.\")\n",
    "                        # Cancel already submitted, not yet running tasks\n",
    "                        for f in futures:\n",
    "                            if not f.running() and not f.done():\n",
    "                                f.cancel()\n",
    "                        break\n",
    "\n",
    "                    model_name_at_submit = get_current_model_config()['name'] if get_current_model_config() else \"N/A\"\n",
    "                    future = executor.submit(process_subfolder, subfolder, category, prompt, output_json)\n",
    "                    future_to_subfolder[future] = (subfolder, model_name_at_submit)\n",
    "                    futures.append(future)\n",
    "\n",
    "                # Process completed tasks\n",
    "                for future in as_completed(futures):\n",
    "                    # Check if stop signal received while waiting for results\n",
    "                    if should_stop_processing() and not future.done():\n",
    "                         # Don't wait indefinitely if told to stop\n",
    "                         continue\n",
    "\n",
    "                    subfolder, model_used = future_to_subfolder[future]\n",
    "                    try:\n",
    "                        # Check for cancellation first\n",
    "                        if future.cancelled():\n",
    "                            logging.info(f\"Task for {subfolder} was cancelled.\")\n",
    "                            pbar.update(1)\n",
    "                            continue\n",
    "\n",
    "                        result = future.result()\n",
    "\n",
    "                        # Update progress bar description if model switched\n",
    "                        current_model_name_desc = get_current_model_config()['name'] if get_current_model_config() else \"STOPPED\"\n",
    "                        pbar.set_description(f\"Processing {category} (Model: {current_model_name_desc})\")\n",
    "\n",
    "                        if result == \"ALREADY_PROCESSED\":\n",
    "                            total_skipped += 1\n",
    "                        elif result == \"TOKEN_EXCEEDED\":\n",
    "                            # This status now means \"stop processing\", either current model or all models\n",
    "                            logging.warning(f\"Received stop signal (TOKEN_EXCEEDED) from {subfolder}.\")\n",
    "                            # No need to explicitly cancel here, the should_stop_processing check handles it.\n",
    "                            # Ensure the outer loops break correctly\n",
    "                            all_models_exhausted = True # Make sure flag is set\n",
    "                        elif result == \"INSUFFICIENT_IMAGES\":\n",
    "                            total_insufficient += 1\n",
    "                        else:\n",
    "                            total_processed += 1\n",
    "                            if result:\n",
    "                                total_success += 1\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"处理子文件夹 {subfolder} 时发生错误: {str(e)}\")\n",
    "                        total_processed += 1\n",
    "                    \n",
    "                    # Update progress bar even if there was an error or skip\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Break outer loop if all models are exhausted\n",
    "                    if should_stop_processing():\n",
    "                         logging.warning(\"Breaking processing loop as all models are exhausted.\")\n",
    "                         # Attempt to cancel remaining futures\n",
    "                         for f in futures:\n",
    "                              if not f.done():\n",
    "                                   f.cancel()\n",
    "                         break # Break from the as_completed loop\n",
    "\n",
    "        else: # Sequential processing\n",
    "            for subfolder in subfolders:\n",
    "                # Check before each task\n",
    "                if should_stop_processing():\n",
    "                    logging.warning(\"Stopping sequential processing, models exhausted.\")\n",
    "                    break\n",
    "\n",
    "                current_model_name_desc = get_current_model_config()['name'] if get_current_model_config() else \"STOPPED\"\n",
    "                pbar.set_description(f\"Processing {category} (Model: {current_model_name_desc})\")\n",
    "\n",
    "                try:\n",
    "                    result = process_subfolder(subfolder, category, prompt, output_json)\n",
    "                    if result == \"ALREADY_PROCESSED\":\n",
    "                        total_skipped += 1\n",
    "                    elif result == \"TOKEN_EXCEEDED\":\n",
    "                        logging.warning(f\"Received stop signal (TOKEN_EXCEEDED) from {subfolder}.\")\n",
    "                        all_models_exhausted = True # Ensure flag is set\n",
    "                        break # Stop processing this category\n",
    "                    elif result == \"INSUFFICIENT_IMAGES\":\n",
    "                        total_insufficient += 1\n",
    "                    else:\n",
    "                        total_processed += 1\n",
    "                        if result:\n",
    "                            total_success += 1\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"处理子文件夹 {subfolder} 时发生错误: {str(e)}\")\n",
    "                    total_processed += 1\n",
    "                \n",
    "                # Update progress bar even if there was an error or skip\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Close progress bar for the category\n",
    "        pbar.close()\n",
    "\n",
    "        # Break the category loop if all models exhausted\n",
    "        if should_stop_processing():\n",
    "            print(f\"Stopping processing categories as all models are exhausted.\")\n",
    "            break\n",
    "\n",
    "    # 打印统计信息\n",
    "    stats_msg = (f\"处理完成! 总共处理: {total_processed} 个子文件夹, 成功: {total_success}, \"\n",
    "                f\"失败: {total_processed - total_success}, 跳过(已处理): {total_skipped}, \"\n",
    "                f\"图像不足: {total_insufficient}\")\n",
    "    print(stats_msg)\n",
    "    logging.info(stats_msg)\n",
    "    \n",
    "    # 打印最终数据集状态\n",
    "    dataset_msg = f\"最终数据集包含 {len(dataset)} 条记录，已保存到: {output_json}\"\n",
    "    print(dataset_msg)\n",
    "    logging.info(dataset_msg)\n",
    "    \n",
    "    # 打印Token使用统计\n",
    "    token_msg = f\"Token使用统计: 总计 {total_tokens_used} tokens, 平均每请求 {total_tokens_used/total_requests if total_requests > 0 else 0:.1f} tokens\"\n",
    "    print(token_msg) \n",
    "    logging.info(token_msg)\n",
    "    \n",
    "    # 打印费用估算\n",
    "    avg_input_output_rate = (initial_model_config.get(\"input_rate_per_k\", 0.0015) + initial_model_config.get(\"output_rate_per_k\", 0.0045)) / 2  # 假设输入输出token比例接近1:1\n",
    "    cost_estimate = (total_tokens_used / 1000) * avg_input_output_rate\n",
    "    cost_msg = f\"费用估算: ¥{cost_estimate:.4f} (按平均费率计算)\"\n",
    "    print(cost_msg)\n",
    "    logging.info(cost_msg)\n",
    "\n",
    "def main():\n",
    "    # 使用命令行参数解析\n",
    "    parser = argparse.ArgumentParser(description=\"生成场景变化描述并构建数据集\")\n",
    "    parser.add_argument(\"--dataset_path\", type=str, default=\"/data/IceInPot/datasets/fmow-rgb/train-resized\",\n",
    "                        help=\"数据集根目录路径\")\n",
    "    parser.add_argument(\"--categories\", type=str, nargs=\"+\", default=None,\n",
    "                        help=\"要处理的类别，不指定则使用预设的CATEGORIES列表\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"/data/IceInPot/gzb_check/output_json\",\n",
    "                        help=\"输出数据集JSON文件目录路径\")\n",
    "    parser.add_argument(\"--parallel\", action=\"store_true\", default=True, # Keep default as True, but user can override\n",
    "                        help=\"是否使用并行处理 (Concurrency controlled by model config)\")\n",
    "    parser.add_argument(\"--no-parallel\", dest='parallel', action='store_false',\n",
    "                        help=\"禁用并行处理\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not MODEL_CONFIGS:\n",
    "         print(\"Error: MODEL_CONFIGS list is empty. Cannot run without models defined.\")\n",
    "         return\n",
    "\n",
    "    # 自定义变化描述提示词\n",
    "    custom_prompt = \"You are looking at a time series of remote sensing images of the same location. \"\n",
    "    \n",
    "    print(f\"开始处理数据集: {args.dataset_path}\")\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # 使用命令行参数指定的类别或预设类别列表\n",
    "    categories_to_process = args.categories if args.categories else CATEGORIES\n",
    "    print(f\"将处理以下类别: {', '.join(categories_to_process)}\")\n",
    "\n",
    "    print(\"Model Configurations:\")\n",
    "    for config in MODEL_CONFIGS:\n",
    "        print(f\"  - Name: {config['name']}, Token Limit: {config['token_limit']}, Max Workers: {config['max_workers']}, \"\n",
    "              f\"Interval: {config.get('request_interval', 'N/A')}s, \"\n",
    "              f\"Rates (In/Out): ¥{config.get('input_rate_per_k', 'N/A')}/¥{config.get('output_rate_per_k', 'N/A')} per 1k\")\n",
    "\n",
    "    # Initial concurrency is set globally based on the first model\n",
    "    print(f\"并发设置: {'启用' if args.parallel else '禁用'}. Initial max workers: {current_max_workers}\")\n",
    "    logging.info(f\"开始处理数据集: {args.dataset_path}, Models: {json.dumps(MODEL_CONFIGS)}\")\n",
    "\n",
    "    # 为每个类别处理数据集并保存到单独的JSON文件\n",
    "    for category in categories_to_process:\n",
    "        # 为当前类别生成输出JSON文件路径\n",
    "        output_json = os.path.join(args.output_dir, f\"{category}_changes.json\")\n",
    "        print(f\"\\n处理类别: {category}, 输出文件: {output_json}\")\n",
    "        logging.info(f\"开始处理类别: {category}, 输出文件: {output_json}\")\n",
    "        \n",
    "        # 清空全局token统计，为新类别重置模型状态\n",
    "        global total_tokens_used, total_requests, current_model_index, all_models_exhausted, model_token_usage\n",
    "        total_tokens_used = 0\n",
    "        total_requests = 0\n",
    "        current_model_index = 0\n",
    "        all_models_exhausted = False\n",
    "        model_token_usage = {config[\"name\"]: 0 for config in MODEL_CONFIGS}\n",
    "        \n",
    "        # 处理单个类别\n",
    "        process_dataset(\n",
    "            dataset_path=args.dataset_path, \n",
    "            categories=[category], # 传递单一类别列表\n",
    "            prompt=custom_prompt, \n",
    "            output_json=output_json,\n",
    "            use_parallel=args.parallel\n",
    "        )\n",
    "        \n",
    "        print(f\"完成类别 {category} 的处理\")\n",
    "        logging.info(f\"完成类别 {category} 的处理\")\n",
    "        \n",
    "        # 如果所有模型都已用尽，退出循环\n",
    "        if all_models_exhausted:\n",
    "            print(\"所有模型已达到Token限制，停止处理剩余类别\")\n",
    "            logging.warning(\"所有模型已达到Token限制，停止处理剩余类别\")\n",
    "            break\n",
    "    \n",
    "    print(\"所有类别处理完成！\")\n",
    "    logging.info(\"所有类别处理完成！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() # Wrap execution in a main function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
